% CNN Section

\subsection{Convolutional Neural Network (CNN)}
\begin{itemize}
    \item \textbf{Description:} Convolutional Neural Networks (CNNs) are deep learning models designed to process data with a grid-like topology, such as images. They use convolutional layers to automatically learn spatial hierarchies of features from input images.
    \item \textbf{Hyperparameters:} Number of layers, filter sizes, learning rate, batch size, dropout rate, optimizer.
    \item \textbf{Implementation:} Implemented using PyTorch's \texttt{nn.Module} and trained with the Adam optimizer.
\end{itemize}

\subsection{CNN Preprocessing}
\begin{itemize}
    \item \textbf{Reshaping:} Images were reshaped from a 100*100*3 format to 3*100*100, that means 3 2d vectors for each color channel (R, G B).
    \item \textbf{Feature extraction:} Color histograms were computed into a single vector of size 768 (3* 256 bins per channel) for each image.
    \item \textbf{Normalization:} Pixel values were normalized using the l2 norm
          $\|x\|_2 = \sqrt{x_1^2 + x_2^2 + ... + x_n^2}$.
\end{itemize}

\subsection{Neural Network Structure}
\begin{minted}[fontsize=\small]{python}
def get_image_features_from_images(
    images: np.ndarray,
) -> np.ndarray:
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()

        # Define the relu function
        self.relu_fn = nn.ReLU()

        # Define flannten function
        self.flatten = nn.Flatten()

        # Define dropout function
        self.dropout_fn = nn.Dropout(0.5)

        # Image has shape: 3 x 100 x 100

        # Layer 1
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)
        self.bn1_conv = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 32 x 50 x 50

        # Layer 2
        self.conv2 = nn.Conv2d(
            in_channels=32, out_channels=64, kernel_size=3, padding=1
        )
        self.bn2_conv = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 64 x 25 x 25

        # Layer 3
        self.conv3 = nn.Conv2d(
            in_channels=64, out_channels=128, kernel_size=3, padding=1
        )
        self.bn3_conv = nn.BatchNorm2d(128)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 128 x 12 x 12

        # Neural Network Layers (Fully Connected Layers)
        # The input is the output of the last convolutional layer
        self.fc1 = nn.Linear(128 * 12 * 12, 512)
        self.bn1_fc = nn.BatchNorm1d(512)
        self.dropout1 = nn.Dropout(0.5)

        self.fc2 = nn.Linear(512, 256)
        self.bn2_fc = nn.BatchNorm1d(256)
        self.dropout2 = nn.Dropout(0.5)

        self.output_layer = nn.Linear(256, 5)  # Assuming 5 output classes

    def forward(self, x):
        # Convolutional layers
        x = self.pool1(self.relu_fn(self.bn1_conv(self.conv1(x))))
        x = self.pool2(self.relu_fn(self.bn2_conv(self.conv2(x))))
        x = self.pool3(self.relu_fn(self.bn3_conv(self.conv3(x))))

        # Flatten
        x = self.flatten(x)

        # Fully connected layers
        x = self.dropout_fn(self.relu_fn(self.bn1_fc(self.fc1(x))))
        x = self.dropout_fn(self.relu_fn(self.bn2_fc(self.fc2(x))))
        x = self.output_layer(x)
        return x
\end{minted}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{images/neural_network.png}
    \caption{Neural Network Structure}
    \label{fig:neural_network}
\end{figure}

\subsubsection{CNN hyperparameter tuning}

\textbf{Parameter Tuned:} Learning rate (see Table~\ref{tab:cnn-lr-tuning} and Figure~\ref{fig:cnn-lr})

The peak validation accuracy was achieved at a learning rate of 0.0001. After this point, the accuracy slightly decreased, indicating that lowering the learning rate further may have led to a steady decrease in accuracy.


% CNN learning rate tuning results
\begin{table}[h]
    \centering
    \caption{CNN Validation Accuracy for Different Learning Rates}
    \label{tab:cnn-lr-tuning}
    \begin{tabular}{cc}
        \toprule
        Learning Rate & Validation Accuracy (\%) \\
        \midrule
        0.1           & 74.8                     \\
        0.01          & 76.1                     \\
        0.001         & 74.5                     \\
        0.0001        & 80.0                     \\
        0.00001       & 79.9                     \\
        \bottomrule
    \end{tabular}
\end{table}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/cnn_lr.png}
    \caption{Validation accuracy for different learning rates.}
    \label{fig:cnn-lr}
\end{figure}


\textbf{Parameter Tuned:} Epochs (see Table~\ref{tab:cnn-tuning}, Table~\ref{tab:cnn-loss-epochs}, and Figure~\ref{fig:cnn-loss})

The loss is in a steady decline, indicating that the model is learning effectively. The model was stopped at 10 epochs to prevent overfitting.


% CNN training loss per epoch
\begin{table}[h]
    \centering
    \caption{CNN Training Loss for Different Epochs}
    \label{tab:cnn-loss-epochs}
    \begin{tabular}{cc}
        \toprule
        Epoch & Training Loss \\
        \midrule
        1     & 0.73          \\
        2     & 0.52          \\
        3     & 0.42          \\
        4     & 0.34          \\
        5     & 0.30          \\
        6     & 0.24          \\
        7     & 0.20          \\
        8     & 0.17          \\
        9     & 0.14          \\
        10    & 0.13          \\
        \bottomrule
    \end{tabular}
\end{table}

% CNN training loss curve
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/epoch_loss.png}
    \caption{Training loss per epoch for the CNN model.}
    \label{fig:cnn-loss}
\end{figure}


\subsubsection{Confusion Matrix for CNN (Figure~\ref{fig:cnn-cm})}
For this confusion matrix, the CNN model was trained for 10 epochs with a learning rate of 0.0001.

% CNN confusion matrix
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/cnn_confusion_matrix.png}
    \caption{Confusion matrix for the CNN model.}
    \label{fig:cnn-cm}
\end{figure}